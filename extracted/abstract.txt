{
    "Abstract": {
        "cnn3": [
            {
                "text": "CNNs are considered the gold standard in deep learning-based image analysis. For instance, in biomedical imaging, they overcome the drawbacks of subjective analysis in the semi-quantitative visual inspection of samples (Gurcan et al., 2009), and they support experts\n\n\n\n\nFigure 6: Class activation maps (CAMs) of a FF trained CNN show which image regions are considered beneficial (yellow) or deleterious (pink) by the network for making its prediction. (A), (C), (E), and (G) display four input images. (B), (D), (F), and (H) are their corresponding CAMs. All examples are from a network with 16 convolutional neurons per layer, filter size 5x5, and trained with a batch size of 50.\n\n\n\nFigure 7: Class activation maps show that the different layers of the FF-trained CNN provide similar, but yet distinguishable information. (A) shows the CAM obtained from considering both layer 2 and layer 3 together. (B) and (C) show the CAMs obtained respectively only from layer 2 and layer 3.\n\nduring their daily clinical routine by reducing their workload (Shmatko et al., 2022). Furthermore, their exploitation of the spatial information within images makes them suitable for the deployment of explainable AI tools (such as class activation maps), which highlight the image regions contributing most significantly to the classification outcome. Our implementation of FF trained CNN shows that with the right choice of hyperparameters, this technique is competitive with backpropagation. These results were obtained without implementing all the possible and suggested optimizations such as enforcing symmetry of the loss function (Lee and Song, 2023) or choosing hard, i.e. easily confused, labels for the negative data set, as suggested by Hinton (2022). We propose that our work shows the\n\n13\n\nSCODELLARO, KULKARNI, ALVES AND SCHR\u00d6TER\n\npotential of FF trained CNNs to address real world computer vision problems. An open question remains if this technique will supersede BP in specific applications. We believe that this potential exists, especially in the cases of neuromorphic hardware and unsupervised learning.\n\nA better understanding of the FF training will however also expand our understanding of the generic concept of neuronal information processing in all its breadth from biological systems to reservoir computing. The demonstrated capability to implement class activation maps offers an initial insight into these research topics. Achieving deeper insights will also mean to understand how the two innovations of FF, providing positive and negative labels and computing a locally defined goodness parameter, contribute to its success individually and synergetically (Tosato et al., 2023). Moreover, a better understanding why it is beneficial to exclude the first layer during the goodness computation (c.f. Appendix B) would be desirable. Subsequent work on FF training should also address its ability to train deeper networks, most likely expanding on the work of Lorberbom et al. (2023). Also the ability of FF training to work with larger and more complex data sets needs to be explored. Finally, its connection to biological neuronal systems (Ororbia and Mali, 2023; Ororbia, 2023) seems a promising research direction.",
                "similarity": 0.8076777458190918
            }
        ],
        "cnn4": [
            {
                "text": "Convolutional Neural Networks (CNNs) have revolutionized the field of deep learning, especially in processing grid-like data structures such as images [1]. Their effectiveness in tasks like image classification [2, 3], object detection [4, 5], semantic segmentation [6, 7] and image generation [8] stem from their ability to effectively learn spatial features. Convolutional layers, using filters or kernels, capture local patterns and extract features from input images. One important feature of convolutional layers is the shared weights implemented by kernels. This allows for efficient deep learning on images, as using only fully connected layers for such tasks would result in unfathomable numbers of parameters. Pooling layers, like max pooling and average pooling, reduce the spatial dimensions of these features, helping the network to focus on the most significant aspects.\n\nDespite their popularity, CNNs face challenges in computational efficiency and adaptability. There have been several convolutional neural network architectures that have been proposed that are aimed at efficiency. Some of such architectures include MobileNet [12] and EfficientNet [13]. However, such traditional CNNs, with fixed architectures and number of parameters, may not perform uniformly across different types of input data with varying levels of complexity.\n\nNeural Architecture Search (NAS), a method for selecting optimal neural network architectures, has been a response to this challenge. NAS aims to obtain the best model for a specific task under certain constraints [14]. However, NAS is often resource-intensive due to the need to train multiple candidate models. in order to determine the optimal architecture. It is estimated that the carbon emission produced when using NAS to train a transformer model can amount to five times the lifetime carbon emissions of an average car [15]. This highlights the importance of finding suitable\n\narchitectures for neural networks, yet also points to the limitations of current approaches in terms of static structure and proneness to over-parameterization.\n\nSelf Expanding Neural Networks (SENN), introduced in [9], offer a promising direction. Inspired by neurogenesis, SENN dynamically adds neurons and fully connected layers to the architecture during training using a natural expansion score (defined in section 2.1) as a criteria to guide this process. This helps overcome the problem of over-parametrization. However, its application has been limited to multilayer perceptrons, with extensions to more practical architectures like CNNs identified as a future research prospect.\n\nOur study aims to develop a Self Expanding Convolutional Neural Network (SECNN), building on the concept of SENN and applying it to modern vision tasks. To the best of our knowledge, there has been no research on Self Expanding CNNs, despite the potential they hold for addressing model efficiency and adaptability in vision tasks. Unlike existing approaches that often require restarting training after modifications or rely on preset mechanisms for expansion, our approach utilizes the natural expansion score for dynamic and optimal model expansion. This research represents a significant step in developing adaptable, efficient CNN models for a variety of vision-related tasks.\n\nThe contributions of this research are as follows:\n\n- Developing a Self Expanding CNN that dynamically determines the optimal model size based on the task, thereby enhancing efficiency.\n- Eliminating the need to train multiple CNN models of varying sizes by allowing for the extraction of checkpoints at diverse complexity levels.\n- Eliminating the need to restart the training process after expanding the CNN model.",
                "similarity": 0.8390400409698486
            }
        ],
        "cnn5": [
            {
                "text": "Image classification is a core task in computer vision and machine learning, where the goal is to assign one or more labels to an image based on its content. It serves as the foundation for numerous real-world applications, such as facial recognition, autonomous driving, medical diagnosis, and smart manufacturing [1], [2]. The CIFAR-10 dataset [3], introduced by Krizhevsky and Hinton [3], has become a standard benchmark in this field. It consists of 60,000 color images divided into 10 distinct classes, each representing a unique object category, such as airplanes, automobiles, birds, and cats. The dataset is balanced, with an equal number of samples for each class, making it an ideal testbed for evaluating classification algorithms. The relatively small size of the dataset allows for efficient experimentation while retaining sufficient complexity to challenge advanced models.\n\nConvolutional Neural Networks (CNNs) have been the backbone of most state-of-the-art models in image classification. CNNs excel in extracting hierarchical spatial features from images, enabling them to capture low-level edges and textures in earlier layers and high-level semantic features in deeper layers. This hierarchical feature extraction capability, combined with their translational invariance, makes CNNs particularly well-suited for visual tasks. However, achieving high accuracy on the CIFAR-10 dataset presents several challenges. First, the dataset's small image resolution of 32x32x3 limits the amount of information and detail that can be extracted, thereby constraining the model's capacity to distinguish between similar classes. Second, the limited size of the dataset increases the risk of overfitting, especially for deeper networks with a large number of parameters. This necessitates the use of effective regularization techniques and data augmentation strategies. Finally, there is a need for a robust network design that strikes a balance between depth, parameter efficiency, and regularization to achieve both high accuracy and generalization.\n\nThis paper aims to address these challenges by proposing an enhanced CNN architecture specifically designed for CIFAR-10 image classification. As suggested in [4], by integrating deeper convolutional blocks, batch normalization to stabilize training, and dropout to mitigate overfitting, the proposed model effectively extracts rich hierarchical features while maintaining robustness. Experimental results demonstrate that this architecture achieves superior performance, highlighting its potential for tackling small-scale yet complex image classification tasks.\n\nIn this paper, we address these challenges by proposing an enhanced CNN architecture with deeper convolutional layers, batch normalization for stable training, and dropout regularization to mitigate overfitting. Our model surpasses standard CNN baselines in accuracy, highlighting the importance of architectural refinement in deep learning applications.",
                "similarity": 0.8322737216949463
            }
        ],
        "cnn1": [
            {
                "text": "Artificial Neural Networks (ANNs) are computational processing systems of which are heavily inspired by way biological nervous systems (such as the human brain) operate. ANNs are mainly comprised of a high number of interconnected computational nodes (referred to as neurons), of which work entwine in a distributed fashion to collectively learn from the input in order to optimise its final output.\n\nThe basic structure of a ANN can be modelled as shown in Figure 1. We would load the input, usually in the form of a multidimensional vector to the input layer of which will distribute it to the hidden layers. The hidden layers will then make decisions from the previous layer and weigh up how a stochastic change within itself detriments or improves the final output, and this is referred to as the process of learning. Having multiple hidden layers stacked upon each-other is commonly called deep learning.\n\n2      Keiron O'Shea et al.\n\n```mermaid\ngraph LR\nsubgraph Input Layer\nI1[Input 1]\nI2[Input 2]\nI3[Input 3]\nI4[Input 4]\nend\nsubgraph Hidden Layer\nH1\nH2\nend\nsubgraph Output Layer\nO[Output]\nend\nI1 --> H1\nI1 --> H2\nI2 --> H1\nI2 --> H2\nI3 --> H1\nI3 --> H2\nI4 --> H1\nI4 --> H2\nH1 --> O\nH2 --> O\n```\n\nFig. 1: A simple three layered feedforward neural network (FNN), comprised of a input layer, a hidden layer and an output layer. This structure is the basis of a number of common ANN architectures, included but not limited to Feedforward Neural Networks (FNN), Restricted Boltzmann Machines (RBMs) and Recurrent Neural Networks (RNNs).\n\nThe two key learning paradigms in image processing tasks are supervised and unsupervised learning. Supervised learning is learning through pre-labelled inputs, which act as targets. For each training example there will be a set of input values (vectors) and one or more associated designated output values. The goal of this form of training is to reduce the models overall classification error, through correct calculation of the output value of training example by training.\n\nUnsupervised learning differs in that the training set does not include any labels. Success is usually determined by whether the network is able to reduce or increase an associated cost function. However, it is important to note that most image-focused pattern-recognition tasks usually depend on classification using supervised learning.\n\nConvolutional Neural Networks (CNNs) are analogous to traditional ANNs in that they are comprised of neurons that self-optimise through learning. Each neuron will still receive an input and perform a operation (such as a scalar product followed by a non-linear function) - the basis of countless ANNs. From the input raw image vectors to the final output of the class score, the entire of the network will still express a single perceptive score function (the weight). The last layer will contain loss functions associated with the classes, and all of the regular tips and tricks developed for traditional ANNs still apply.\n\nThe only notable difference between CNNs and traditional ANNs is that CNNs are primarily used in the field of pattern recognition within images. This allows us to encode image-specific features into the architecture, making the network\n\nIntroduction to Convolutional Neural Networks    3\n\nmore suited for image-focused tasks - whilst further reducing the parameters required to set up the model.\n\nOne of the largest limitations of traditional forms of ANN is that they tend to struggle with the computational complexity required to compute image data. Common machine learning benchmarking datasets such as the MNIST database of handwritten digits are suitable for most forms of ANN, due to its relatively small image dimensionality of just 28 \u00d7 28. With this dataset a single neuron in the first hidden layer will contain 784 weights (28 \u00d7 28 \u00d7 1 where 1 bare in mind that MNIST is normalised to just black and white values), which is manageable for most forms of ANN.\n\nIf you consider a more substantial coloured image input of 64 \u00d7 64, the number of weights on just a single neuron of the first layer increases substantially to 12,288. Also take into account that to deal with this scale of input, the network will also need to be a lot larger than one used to classify colour-normalised MNIST digits, then you will understand the drawbacks of using such models.",
                "similarity": 0.8390400409698486
            }
        ],
        "cnn2": [
            {
                "text": "",
                "similarity": 0.8390400409698486
            }
        ]
    }
}